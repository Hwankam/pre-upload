---
layout : post
title : PRML_study_chap_1_basic
subtitle : 
date : 2021-09-10
#categories:
tags : [datascience, PRML1]
toc_sticky : true
use_math : true
comments: true
---



### PRML introduction

training phase, learning phase, training data, test set, generalization, .....

generalization : 가지고 있는 것은 표본 밖에 없기 때문에, 학습된 모델이 얼마나 general하게 사용될 수 있는지 그 성능을 파악하는 것은 중요하다. 

feature extraction : 학습 이전에 feature를 미리 확인하고, 학습에 적합한(필요한) feature를 추출해낸 후, 학습에 용이한 형태로 변환하는 것. 핵심적인 feature를 유지한 채로 computing 속도 향상을 위해

supervised learning : training data가 input, output(target) vector 모두 가지는 경우

unsupervised learning : training data가 input vector만 가지는 경우 e.g. clustering, density estimation, project high-dimensional space into lower dimension for the purpose of visualization.

reinforcement learning : 보상을 최대화 하기 위한 적절한 행동을 찾아내는 문제, 그렇게 하도록 학습시키는 문제, 학습에서 최적 output을 미리 받는 것은 아니고 trial and error를 통해 이를 찾아내는데 관심을 가지고 있다.

#### Example : Polynomial Curving Fitting

polynomial curving fitting에서는 polynomial의 차수를 결정하는 것이 중요하다. 

아래 그림이 이를 잘 설명하고 있다. 

<img src='{{"/assets/img/PRML_basic2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

차수가 커질수록 추정하는 polynomial이 flexible 하며 oscillation을 잘 잡아낼 수 있다. 


<br>

polynomial 의 차수와 해당 계수 크기 사이의 관계 또한 존재한다. 아래 그림을 보자

<img src='{{"/assets/img/PRML_basic3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림에서 알 수 있듯, M의 크기가 커지면 즉 polynomial의 차원이 클수록 추정 계수값이 커진다는 것을 알 수 있다.(왜? 차수가 커지면서 더욱 복잡한 모형을 표현할 수 있게 되고, 데이터 하나하나에 곡선이 맞춰지다보니 곡선 자체가 복잡해지고 계수값은 매우 커진다). 또한 모델이 복잡하다보니 range 양 끝 부분은 더더욱 oscillation이 더욱 심해진다. 

<br>

data size 와 fitting : data size 가 클수록 overfitting 문제를 해결하는 데에 용이하다. 또한 data size가 클수록 복잡한 모델의 표현이 가능해진다. LSE 혹은 MLE는 모델을 먼저 설정하기 때문에 오버피팅의 문제를 피할수 없고, 모델 내의 파라미터 개수(모델 복잡성)의 5배 혹은 10배 이상의 데이터 개수가 있어야 학습이 용이하다. 

그런데 데이터의 개수에 따라서 모델의 파라미터를 한정짓는다는 것은 일반적인 관점에서 볼때 한계가 있다. 우리는 모델링을 할 때 우선적으로 문제를 정의하고 그에 맞는 모델을 설정해야하기 때문이다. 베이지안의 관점은 좀 다르다. 데이터의 사이즈에 따라서 적절한 파라미터 개수를 조절한다. 

기존의 관점을 고수할 때, 데이터 사이즈가 작은 경우 복잡한 모델을 어떻게 만들어 낼 수 있을까?  =>> regularization(shrinkage/ ridge and lasso / weight decay in neural net) 


지금까지의 논의들(모델의 차수, 오버피팅, data size) 등은 결국 model complexity를 설명하기 위한 것이다. taking data -> model complexity 설정 -> train / validation set  (이 문장은 별 필요 없는 듯)




#### 1.2 Probability Theory

bayes' theorem , prior, posterior, independent, change of variable, CDF, Expectation, Covariance, bayesian, MLE, MCMC, Monte Carlo metod, variational Bayes..




1. change of variable

$$
p_y(y) = p_x(x)|\frac{dx}{dy}|
$$

2. MLE

    관찰된 데이터 셋의 확률을 가장 크게 만드는 모수 추정량

3. frequentist error bar : bootstrap

4. bayesian vs frequentist
    
    동전 던지기 결과 100번 모두 앞면이 나왔다면? frequentist 들은 나온 표본을 가지고 판정하는데 중점을 둔다. 따라서 앞면이 나올 확률를 나타내는 모수 p 에 대한 mle는 1. but bayesian 관점에서는 사전분포를 고려하기 때문에 이렇게 극단적인 값은 안나올 것.  

##### 1.2.4 Gaussian distribution



in more complex model, the bias problem associated with ML will be more severe. 

(가우시안 분포에서 분산에 대한 ml 추정량은 True 값에 대해 bias estimator 이지만 N이 커지면 문제가 되지 않는다. 그러나 더욱 복잡한 모델에서는 고려해야 할 파라미터가 많아지고 bias estimator에 대해 문제가 생길 수 있다. )

In fact, the issue of bias in ML lies at the root of the over-fitting problem(가우시안 모델에서 ML방식을 사용할 경우 분산을 과소추정하게 되는데 이러한 경우 식 자체의 계수값을 작게 만들 수 있고(linear 모델에서 오버피팅하는 경우 계수값이 커지는 것과 반대?) 또한 ML 방식의 경우 현재 가지고 있는 데이터만을 고려하는 추정방식이므로 일반적인 경향성(generalization)을 찾지 못하고 현재 가지고 있는 데이터에만 포커스를 맞춰 적합신다는 측면에서 오버피팅과 결을 같이한다고 볼 수 있다 ).



##### 1.2.5 Curving fitting re-visited

ML 추정량을 통해 찾은 분포를 통해 확률적 모델을 만들 수 있다. => predictive distribution.
$$
p(t|x,W_{ML},\beta_{ML}) = N(t|y(x,W_{ML}),\beta_{ML}^{-1})
$$
또한 베이지안 관점에서 베이지안 추정량을 찾을 수 있다.

posterior에 대해 제곱손실오차에 대한 베이즈기대손실을 최소화하는 추정량은 사후평균 

posterior에 대해 절댓값 오차에 대한 베이즈기대손실을 최소화하는 추정량은 사후분포의 중앙값

Maximize a Posterior(사후분포의 mode, MAP)

MAP에 대해 "maximizing the posterior distribution is equivalent to minimizing the regularized sum of squares error function "

이에 대한 증명은 아래와 같다



<img src='{{"/assets/img/PRML_basic1.jpg"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



##### 1.2.6 Bayesian curving fitting



앞 장에서 prior -> posterior -> MAP를 찾았지만 사실 이건 점추정 방식.

완전한 Bayesian approach를 위해서는 marginalization을 통해 모수 값을 제거하는 것이 필요하다.

predictive distribution(bayesian treatment)
$$
p(t|x,\boldsymbol X,\boldsymbol t) = \int p(t|x,W)p(W|\boldsymbol X,\boldsymbol t)dW
$$


curving-fitting의 문제에서 posterior는 analytic하게 가우시안 분포를 따르게 되며 이때 predictive distribution 또한 analytic하게 가우시안 분포를 따르게 된다. 또한 그 분포의 평균과 분산이 모두 new test point x에 의존한다.



#### 1.3 Model Selection

최적 모델을 선정하기 위해서는 파라미터의 개수/종류 선택이 중요하다. 만약 파라미터 개수가 늘어날수록 최적 파라미터의 조합을 찾는 데에도 많은 비용이 필요하다.

그리고 최적모델인지 아닌지 평가하기 위해 train - validation - test 셋으로 나누는 것이 중요하다. 그러나 데이터가 부족한 경우 Cross - validation(leave - one- out)이 유용하다. 그러나 이 방법은 training의 횟수가 늘어나기 때문에 시간/ 계산 비용이 많이 든다.  



AIC, BIC => information = (in-sample-performance) + (penalty)



#### 1.4 The Curse of Dimensionality

추정해야할 파라미터 개수가 더 많아질수록 더 많은 양의 데이터가 요구된다. 데이터 셋의 차원이 D이면 M차원의 polynomial에 대해서 모델의 계수 number 는 D^M이 된다.

''차원의 저주'' 문제는 모델 학습에 중요한 문제이지만, 데이터 차원 보다 더 고려해야할 부분은

1. 학습에 중요한 변수는 무엇인가? 그리고 이는 일반적으로 몇개 안된다(전체 변수에 비해)
2. real data가 보여주는 smoothing property 때문에 local 회귀 같은 것으로 예측을 잘 할 수 있다.  






